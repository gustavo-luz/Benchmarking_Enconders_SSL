{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Training a Custom Encoder on the DAGHAR Dataset\n",
    "\n",
    "\n",
    "In this notebook, we will train a custom encoder from scratch on the DAGHAR KuHAR dataset. The encoder can be **any** `torch.nn.Module` \n",
    "\n",
    "To do this, we will first define your model by:\n",
    "1. Define the encoder. We going to call this encoder as `backbone`.\n",
    "2. Calculating the embedding size of the `backbone`. This is necessary to define the head of the model. This is done by passing a dummy input through the `backbone` and checking the output shape.\n",
    "3. Defining the head of the model. We use a standard MLP classifier as the head, whose input size is the embedding size, followed by a hidden layer of 128 units and an output layer with the number of classes in the dataset, that is, 6.\n",
    "4. Creating a `SimpleSupervisedModel` with the `backbone` and the head. The `SimpleSupervisedModel` is a PyTorch Lightning module that receives the `backbone` and the head as arguments and trains the model end-to-end. This model simply forwards the input through the `backbone`, flattens the output, and passes it through the head to get the logits, whose loss is calculated using the cross-entropy loss.\n",
    "\n",
    "Note that, for sake of reproducibility, you should only change parts related to loading the encoder and defining the head of the model. The rest of the code should remain the same (or at least very similar) to the one provided in this notebook.\n",
    "\n",
    "**Notes:**\n",
    "1. The `backbone` should have a `forward` method that takes a batch of time series as input and returns the output of the encoder (embeddings). Your encoder must accept samples with the shape `(batch_size, channels, steps)`, where `channels` is the number of channels in the time series and `steps` is the number of time steps. For DAGHAR dataset, `channels=6` and `steps=60`. **Thus, your encoder should accept samples with the shape `(batch_size, 6, 60)`.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "from minerva.models.nets.base import SimpleSupervisedModel\n",
    "\n",
    "import torchmetrics\n",
    "from minerva.models.nets.time_series.cnns import CNN_PF_Backbone\n",
    "from minerva.data.data_modules.har import MultiModalHARSeriesDataModule\n",
    "from minerva.models.loaders import FromPretrained\n",
    "from minerva.models.nets.base import SimpleSupervisedModel\n",
    "from minerva.models.nets.mlp import MLP\n",
    "from minerva.analysis.metrics.balanced_accuracy import BalancedAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mactivation_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.nn.modules.activation.ReLU'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    A multilayer perceptron (MLP) implemented as a subclass of nn.Sequential.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This MLP is composed of a sequence of linear layers interleaved with ReLU activation\u001b[0m\n",
      "\u001b[0;34m    functions, except for the final layer which remains purely linear.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Example\u001b[0m\n",
      "\u001b[0;34m    -------\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> mlp = MLP(10, 20, 30, 40)\u001b[0m\n",
      "\u001b[0;34m    >>> print(mlp)\u001b[0m\n",
      "\u001b[0;34m    MLP(\u001b[0m\n",
      "\u001b[0;34m        (0): Linear(in_features=10, out_features=20, bias=True)\u001b[0m\n",
      "\u001b[0;34m        (1): ReLU()\u001b[0m\n",
      "\u001b[0;34m        (2): Linear(in_features=20, out_features=30, bias=True)\u001b[0m\n",
      "\u001b[0;34m        (3): ReLU()\u001b[0m\n",
      "\u001b[0;34m        (4): Linear(in_features=30, out_features=40, bias=True)\u001b[0m\n",
      "\u001b[0;34m    )\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mactivation_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Initializes the MLP with specified layer sizes.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        layer_sizes : Sequence[int]\u001b[0m\n",
      "\u001b[0;34m            A sequence of positive integers indicating the size of each layer.\u001b[0m\n",
      "\u001b[0;34m            At least two integers are required, representing the input and output layers.\u001b[0m\n",
      "\u001b[0;34m        activation_cls : type\u001b[0m\n",
      "\u001b[0;34m            The class of the activation function to use between layers. Default is nn.ReLU.\u001b[0m\n",
      "\u001b[0;34m        *args\u001b[0m\n",
      "\u001b[0;34m            Additional arguments passed to the activation function.\u001b[0m\n",
      "\u001b[0;34m        **kwargs\u001b[0m\n",
      "\u001b[0;34m            Additional keyword arguments passed to the activation function.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises\u001b[0m\n",
      "\u001b[0;34m        ------\u001b[0m\n",
      "\u001b[0;34m        AssertionError\u001b[0m\n",
      "\u001b[0;34m            If fewer than two layer sizes are provided or if any layer size is not a positive integer.\u001b[0m\n",
      "\u001b[0;34m        AssertionError\u001b[0m\n",
      "\u001b[0;34m            If activation_cls does not inherit from torch.nn.Module.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Multilayer perceptron must have at least 2 layers\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mls\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"All layer sizes must be positive integers\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mactivation_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"activation_cls must inherit from torch.nn.Module\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           /workspaces/HIAAC-KR-Dev-Container/Minerva-Dev/minerva/models/nets/mlp.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "MLP??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ID: run_20260127-021414\n",
      "Log dir: ./logs/run_20260127-021414\n"
     ]
    }
   ],
   "source": [
    "# Name of the experiment\n",
    "execution_id = f'run_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "# Directory to save logs\n",
    "log_dir = f\"./logs/{execution_id}\" \n",
    "\n",
    "\n",
    "print(f\"Execution ID: {execution_id}\")\n",
    "print(f\"Log dir: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised training of Custom Encoder\n",
    "\n",
    "In this notebook, we will fine-tune a custom encoder on the DAGHAR dataset. The encoder can be any `torch.nn.Module` that was trained elsewhere and whose checkpoint is available at a `.ckpt` file (saved using `torch.save(model.state_dict())`). This file may contain only the weights from encoder or may contains the weights of the entire model, including the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining the Data Module\n",
    "\n",
    "We will use the `MultiModalHARSeriesDataModule` data module to load the DAGHAR dataset for fine-tuning. This data module loads the data in the format required for fine-tuning, which includes sliding window time series data for each sample. Thus, each sample of the dataset will be a 2-element tuple containing the time series (`6x60`, where 6 is the number of features and 60 is the window size) and the corresponding label.\n",
    "\n",
    "The data module requires the following arguments:\n",
    "- `data_path`: Path to the directory containing the dataset (change it to use other dataset from DAGHAR).\n",
    "- `feature_prefix`: The prefix of the columns containing the features. For each prefix, we will create a different channel with all columns that start with the prefix.\n",
    "- `label`: The name of the column containing the labels.\n",
    "- `features_as_channels`: If True, for each prefix, we will create a different channel with all columns that start with the prefix. If False, we will concatenate all columns with the same prefix into a single channel (the sample will be a tensor of 1x360 instead of 6x60).\n",
    "- `cast_to`: The data type to cast the features (float32).\n",
    "- `batch_size`: The batch size for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalHARSeriesDataModule(data_path=/workspaces/HIAAC-KR-Dev-Container/shared_data/daghar/standardized_view/KuHar, batch_size=64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = MultiModalHARSeriesDataModule(\n",
    "    data_path=\"/workspaces/HIAAC-KR-Dev-Container/shared_data/daghar/standardized_view/KuHar/\",\n",
    "    feature_prefixes=[\"accel-x\", \"accel-y\", \"accel-z\", \"gyro-x\", \"gyro-y\", \"gyro-z\"],\n",
    "    label=\"standard activity code\",\n",
    "    features_as_channels=True,\n",
    "    cast_to=\"float32\",\n",
    "    batch_size=64,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "data_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining the Supervised Model\n",
    "\n",
    "We first going to create the `backbone` of our model, that is our encoder. To do this, we:\n",
    "1. Copy-and-paste the code of the encoder in the cell below. Should be a `torch.nn.Module` or equivalent. You should copy all code necessary to define the encoder, including imports, class definition, the `forward` method, and any other methods or classes that are necessary to define the encoder.\n",
    "\n",
    "**NOTE**: \n",
    "1. Your encoder **must accept samples with the shape `(batch_size, 6, 60)`**. This is the shape of the samples in the DAGHAR dataset. If your encoder does not accept samples with this shape, you should modify it to accept samples with this shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Definition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from minerva.models.nets.tnc import RnnEncoder,TSEncoder\n",
    "from minerva.models.nets.mlp import MLP\n",
    "from minerva.models.nets.base import SimpleSupervisedModel\n",
    "from minerva.models.nets.time_series.resnet import _ResNet1D, ResNetSEBlock\n",
    "from minerva.models.nets.time_series.cnns import CNN_PF_Backbone\n",
    "from minerva.models.nets.time_series.imu_transformer import _IMUTransformerEncoder\n",
    "from minerva.models.nets.lfr_har_architectures import HARSCnnEncoder\n",
    "\n",
    "# backbone = CNN_PF_Backbone(include_middle=True) #768\n",
    "\n",
    "backbone = HARSCnnEncoder(dim=2304,input_channel= 6,inner_conv_output_dim= 1280) #2304\n",
    "\n",
    "# backbone = _ResNet1D(input_shape= (6, 60),residual_block_cls=ResNetSEBlock) #64\n",
    "\n",
    "# backbone = RnnEncoder(\n",
    "#         hidden_size=100,\n",
    "#         in_channel=6,\n",
    "#         encoding_size=320,\n",
    "#         bidirectional=True,\n",
    "#         num_layers=1,\n",
    "#         dropout=0,\n",
    "#         cell_type='GRU'\n",
    "#         # device='cuda',\n",
    "#         permute=True\n",
    "#     ) #320\n",
    "\n",
    "# backbone = TSEncoder(input_dims=6, output_dims=320, hidden_dims=64, depth=10,permute=True) #320\n",
    "\n",
    "# backbone = _IMUTransformerEncoder(\n",
    "#         input_shape= (6, 60),\n",
    "#         transformer_dim = 64,\n",
    "#         encode_position = True,\n",
    "#         nhead= 8,\n",
    "#         dim_feedforward = 128,\n",
    "#         transformer_dropout = 0.1,\n",
    "#         transformer_activation = \"gelu\",\n",
    "#         num_encoder_layers = 6,\n",
    "#     )#64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the head\n",
    "\n",
    "We will define the head of the model. The head is a simple MLP classifier that receives the embeddings from the encoder and outputs the logits. The head consists of a single hidden layer with 128 units and an output layer with the number of classes in the dataset, that is, 6.\n",
    "In order to know the input size of the head, we need to calculate the embedding size of the encoder. This is done by passing a dummy input through the encoder and checking the output shape. Let's pick the first batch of the training data and pass it through the encoder to get the embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DataLoader with shuffle=True\n",
      "O primeiro batch de treino tem shape X=(64, 6, 60) e y=(64,)\n"
     ]
    }
   ],
   "source": [
    "# Pega os dataloader de treino\n",
    "data_module.setup(\"fit\")\n",
    "train_data_loader = data_module.train_dataloader()\n",
    "\n",
    "# Obtem o primeiro batch de treino (64 amostras de 6x60)\n",
    "first_batch = next(iter(train_data_loader))\n",
    "\n",
    "X, y = first_batch\n",
    "print(f\"O primeiro batch de treino tem shape X={tuple(X.shape)} e y={tuple(y.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a single forward pass through the encoder with the first batch of the training data and checked the output shape that is `(64, 16, 60)`. The embedding size is `16 x 30 = 480`, that is, the product of the number of channels and the number of steps in the output of the encoder. Thus, the input layer of our MLP head will have 480 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([64, 6, 60])\n",
      "shape after conv torch.Size([64, 128, 10])\n",
      "shape after view torch.Size([64, 1280])\n",
      "shape after mlp torch.Size([64, 2304])\n",
      "O embedding tem shape (64, 2304)\n",
      "O embedding final, achatado, tem o shape de 2304\n"
     ]
    }
   ],
   "source": [
    "embeddings = backbone(X)\n",
    "# multiplica produtorio de todas as camadas com exceção da de batches\n",
    "mlp_input_shape = embeddings.shape[1] #* embeddings.shape[2]\n",
    "print(f\"O embedding tem shape {tuple(embeddings.shape)}\")\n",
    "print(f\"O embedding final, achatado, tem o shape de {mlp_input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the head of the model in the cell below. The head will be a simple MLP classifier with:\n",
    "- A input layer with 480 units (`mlp_input_shape` variable).\n",
    "- A hidden layer with 128 units.\n",
    "- An output layer with 6 units, that is the number of classes in the dataset (`num_classes` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (0): Linear(in_features=2304, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 6\n",
    "head = MLP([2304, 128, num_classes])\n",
    "head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the SimpleSupervisedModel\n",
    "\n",
    "Finally, we use the `SimpleSupervisedModel` class to create the supervised model with the encoder as the backbone and the MLP classifier as the head. The `SimpleSupervisedModel` class requires the following arguments:\n",
    "- `backbone`: The backbone model (`FromPretrained` model).\n",
    "- `fc`: The head model (the MLP classifier).\n",
    "- `loss_fn`: The loss function to use for training (`CrossEntropyLoss`).\n",
    "- `flatten`: Whether to flatten the input before passing it through the head. Usually, the input is flattened if the backbone outputs a tensor with more than two dimensions.\n",
    "- `train_metrics`: A dictionary where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics.\n",
    "- `val_metrics`: A dictionary where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSupervisedModel(\n",
       "  (backbone): HARSCnnEncoder(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv1d(6, 32, kernel_size=(8,), stride=(1,), padding=(4,), bias=False)\n",
       "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Dropout(p=0.35, inplace=False)\n",
       "      (5): Conv1d(32, 64, kernel_size=(8,), stride=(1,), padding=(4,), bias=False)\n",
       "      (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ReLU()\n",
       "      (8): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (9): Conv1d(64, 128, kernel_size=(8,), stride=(1,), padding=(4,), bias=False)\n",
       "      (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (mlp): Linear(in_features=1280, out_features=2304, bias=True)\n",
       "  )\n",
       "  (fc): MLP(\n",
       "    (0): Linear(in_features=2304, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=6, bias=True)\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleSupervisedModel(\n",
    "    backbone=backbone,\n",
    "    fc=head,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    flatten=True,\n",
    "    train_metrics={\n",
    "        \"acc\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=6),\n",
    "    },\n",
    "    val_metrics={\n",
    "        \"acc\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=6),\n",
    "    },\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters: 3,331,142\n",
      "Trainable model parameters: 3,331,142\n",
      "\n",
      "Backbone:\n",
      "  Total parameters: 3,035,328\n",
      "  Trainable parameters: 3,035,328\n",
      "\n",
      "Classification Head:\n",
      "  Total parameters: 295,814\n",
      "  Trainable parameters: 295,814\n",
      "\n",
      "Verification:\n",
      "Backbone + Head total: 3,331,142\n",
      "Should match total: 3,331,142\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n",
      "MACs: 5,067,648,000.0\n",
      "Parameters: 3,331,142.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1: Measure ALL model parameters (including backbone AND head)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total model parameters: {total_params:,}\")\n",
    "print(f\"Trainable model parameters: {trainable_params:,}\")\n",
    "\n",
    "# Option 2: Measure components separately (as you intended)\n",
    "# Backbone parameters\n",
    "backbone_total = sum(p.numel() for p in model.backbone.parameters())\n",
    "backbone_trainable = sum(p.numel() for p in model.backbone.parameters() if p.requires_grad)\n",
    "\n",
    "# Head parameters (use model.fc, not head variable)\n",
    "head_total = sum(p.numel() for p in model.fc.parameters())\n",
    "head_trainable = sum(p.numel() for p in model.fc.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nBackbone:\")\n",
    "print(f\"  Total parameters: {backbone_total:,}\")\n",
    "print(f\"  Trainable parameters: {backbone_trainable:,}\")\n",
    "\n",
    "print(f\"\\nClassification Head:\")\n",
    "print(f\"  Total parameters: {head_total:,}\")\n",
    "print(f\"  Trainable parameters: {head_trainable:,}\")\n",
    "\n",
    "# Verify the sum matches Option 1\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Backbone + Head total: {backbone_total + head_total:,}\")\n",
    "print(f\"Should match total: {total_params:,}\")\n",
    "\n",
    "from thop import profile\n",
    "evaluation_data = torch.rand(1000, 6, 60, device='cuda')\n",
    "model.to('cuda')\n",
    "macs, params = profile(model, inputs=(evaluation_data,))\n",
    "\n",
    "print(f\"MACs: {macs:,}\")\n",
    "print(f\"Parameters: {params:,}\")\n",
    "\n",
    "# from codecarbon import EmissionsTracker\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 02:14:15] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 02:14:15] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 02:14:15] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 02:14:17] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 02:14:17] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz\n",
      "[codecarbon WARNING @ 02:14:17] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 02:14:17] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 02:14:17] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 02:14:17] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 02:14:17] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 02:14:17]   Platform system: Linux-6.8.0-65-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 02:14:17]   Python version: 3.10.6\n",
      "[codecarbon INFO @ 02:14:17]   CodeCarbon version: 3.2.1\n",
      "[codecarbon INFO @ 02:14:17]   Available RAM : 62.764 GB\n",
      "[codecarbon INFO @ 02:14:17]   CPU count: 12 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 02:14:17]   CPU model: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz\n",
      "[codecarbon INFO @ 02:14:17]   GPU count: 1\n",
      "[codecarbon INFO @ 02:14:17]   GPU model: 1 x NVIDIA RTX 5000 Ada Generation BUT only tracking these GPU ids : ['0']\n",
      "[codecarbon INFO @ 02:14:20] Energy consumed for RAM : 0.000003 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:21] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.455877000000001 W\n",
      "[codecarbon INFO @ 02:14:21] Energy consumed for All CPU : 0.000001 kWh\n",
      "[codecarbon INFO @ 02:14:21] Energy consumed for all GPUs : 0.000007 kWh. Total GPU Power : 25.197161472828384 W\n",
      "[codecarbon INFO @ 02:14:21] 0.000012 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:21] Energy consumed for RAM : 0.000006 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:22] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.533871000000001 W\n",
      "[codecarbon INFO @ 02:14:22] Energy consumed for All CPU : 0.000003 kWh\n",
      "[codecarbon INFO @ 02:14:22] Energy consumed for all GPUs : 0.000014 kWh. Total GPU Power : 24.107848514461747 W\n",
      "[codecarbon INFO @ 02:14:22] 0.000023 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:22] Energy consumed for RAM : 0.000009 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:23] Delta energy consumed for CPU with cpu_load : 0.000002 kWh, power : 10.982364992 W\n",
      "[codecarbon INFO @ 02:14:23] Energy consumed for All CPU : 0.000004 kWh\n",
      "[codecarbon INFO @ 02:14:23] Energy consumed for all GPUs : 0.000021 kWh. Total GPU Power : 23.400481989773414 W\n",
      "[codecarbon INFO @ 02:14:23] 0.000034 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:24] Energy consumed for RAM : 0.000012 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:24] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.101154816000001 W\n",
      "[codecarbon INFO @ 02:14:24] Energy consumed for All CPU : 0.000005 kWh\n",
      "[codecarbon INFO @ 02:14:24] Energy consumed for all GPUs : 0.000027 kWh. Total GPU Power : 23.63436129138841 W\n",
      "[codecarbon INFO @ 02:14:24] 0.000044 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:25] Energy consumed for RAM : 0.000014 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:25] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.059802048 W\n",
      "[codecarbon INFO @ 02:14:25] Energy consumed for All CPU : 0.000006 kWh\n",
      "[codecarbon INFO @ 02:14:25] Energy consumed for all GPUs : 0.000035 kWh. Total GPU Power : 28.335790303838273 W\n",
      "[codecarbon INFO @ 02:14:25] 0.000056 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:26] Energy consumed for RAM : 0.000017 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:26] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.127552392 W\n",
      "[codecarbon INFO @ 02:14:26] Energy consumed for All CPU : 0.000008 kWh\n",
      "[codecarbon INFO @ 02:14:26] Energy consumed for all GPUs : 0.000042 kWh. Total GPU Power : 24.291285168970404 W\n",
      "[codecarbon INFO @ 02:14:26] 0.000067 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:27] Energy consumed for RAM : 0.000020 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:27] Delta energy consumed for CPU with cpu_load : 0.000002 kWh, power : 14.917177663999999 W\n",
      "[codecarbon INFO @ 02:14:27] Energy consumed for All CPU : 0.000010 kWh\n",
      "[codecarbon INFO @ 02:14:27] Energy consumed for all GPUs : 0.000049 kWh. Total GPU Power : 23.8433755728497 W\n",
      "[codecarbon INFO @ 02:14:27] 0.000079 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:28] Energy consumed for RAM : 0.000023 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:28] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.657311688 W\n",
      "[codecarbon INFO @ 02:14:28] Energy consumed for All CPU : 0.000011 kWh\n",
      "[codecarbon INFO @ 02:14:28] Energy consumed for all GPUs : 0.000056 kWh. Total GPU Power : 24.094406286383894 W\n",
      "[codecarbon INFO @ 02:14:28] 0.000089 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:29] Energy consumed for RAM : 0.000026 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:29] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.121331448000001 W\n",
      "[codecarbon INFO @ 02:14:29] Energy consumed for All CPU : 0.000012 kWh\n",
      "[codecarbon INFO @ 02:14:29] Energy consumed for all GPUs : 0.000062 kWh. Total GPU Power : 23.33785038586996 W\n",
      "[codecarbon INFO @ 02:14:29] 0.000100 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:30] Energy consumed for RAM : 0.000028 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:30] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.705624768000002 W\n",
      "[codecarbon INFO @ 02:14:30] Energy consumed for All CPU : 0.000013 kWh\n",
      "[codecarbon INFO @ 02:14:30] Energy consumed for all GPUs : 0.000069 kWh. Total GPU Power : 23.787815999546204 W\n",
      "[codecarbon INFO @ 02:14:30] 0.000111 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:31] Energy consumed for RAM : 0.000031 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:31] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 10.634794496000001 W\n",
      "[codecarbon INFO @ 02:14:31] Energy consumed for All CPU : 0.000015 kWh\n",
      "[codecarbon INFO @ 02:14:31] Energy consumed for all GPUs : 0.000076 kWh. Total GPU Power : 24.032539555955864 W\n",
      "[codecarbon INFO @ 02:14:31] 0.000122 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:32] Energy consumed for RAM : 0.000034 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:32] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.820125 W\n",
      "[codecarbon INFO @ 02:14:32] Energy consumed for All CPU : 0.000016 kWh\n",
      "[codecarbon INFO @ 02:14:32] Energy consumed for all GPUs : 0.000084 kWh. Total GPU Power : 28.09282280340504 W\n",
      "[codecarbon INFO @ 02:14:32] 0.000134 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:33] Energy consumed for RAM : 0.000037 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:33] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.620289 W\n",
      "[codecarbon INFO @ 02:14:33] Energy consumed for All CPU : 0.000017 kWh\n",
      "[codecarbon INFO @ 02:14:33] Energy consumed for all GPUs : 0.000090 kWh. Total GPU Power : 24.50164956507005 W\n",
      "[codecarbon INFO @ 02:14:33] 0.000144 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:34] Energy consumed for RAM : 0.000040 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sb torch.Size([1000, 6, 60])\n",
      "shape after conv torch.Size([1000, 128, 10])\n",
      "shape after view torch.Size([1000, 1280])\n",
      "shape after mlp torch.Size([1000, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:14:34] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.210542904 W\n",
      "[codecarbon INFO @ 02:14:34] Energy consumed for All CPU : 0.000018 kWh\n",
      "[codecarbon INFO @ 02:14:34] Energy consumed for all GPUs : 0.000097 kWh. Total GPU Power : 23.878552022626103 W\n",
      "[codecarbon INFO @ 02:14:34] 0.000155 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:14:34] Energy consumed for RAM : 0.000040 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:14:35] Delta energy consumed for CPU with cpu_load : 0.000000 kWh, power : 8.050757768 W\n",
      "[codecarbon INFO @ 02:14:35] Energy consumed for All CPU : 0.000018 kWh\n",
      "[codecarbon INFO @ 02:14:35] Energy consumed for all GPUs : 0.000104 kWh. Total GPU Power : 48.92625654971029 W\n",
      "[codecarbon INFO @ 02:14:35] 0.000162 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy_mWh</th>\n",
       "      <th>energy_J</th>\n",
       "      <th>emissions_mgCO2eq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>39.28</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.27</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>43.04</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.87</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>41.69</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.79</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>37.76</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.56</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>39.72</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>42.93</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.01</td>\n",
       "      <td>39.89</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      energy_mWh  energy_J  emissions_mgCO2eq\n",
       "2           0.01     39.28               1.07\n",
       "3           0.01     38.27               1.05\n",
       "4           0.01     43.04               1.18\n",
       "5           0.01     38.87               1.06\n",
       "6           0.01     41.69               1.14\n",
       "7           0.01     38.79               1.06\n",
       "8           0.01     37.76               1.03\n",
       "9           0.01     38.56               1.05\n",
       "10          0.01     39.72               1.09\n",
       "11          0.01     42.93               1.17\n",
       "mean        0.01     39.89               1.09\n",
       "std         0.00      1.94               0.05"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "TOTAL_RUNS = 14\n",
    "DISCARD_FIRST = 2\n",
    "DISCARD_LAST = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "tracker = EmissionsTracker(\n",
    "    project_name=\"basic_measurement\",\n",
    "    measure_power_secs=10,\n",
    "    save_to_file=False\n",
    ")\n",
    "\n",
    "try:\n",
    "    for run_id in range(TOTAL_RUNS):\n",
    "        tracker.start_task(f\"measure_inference_{run_id}\")\n",
    "\n",
    "        _ = model(evaluation_data)  # inference\n",
    "\n",
    "        emissions = tracker.stop_task()\n",
    "\n",
    "        energy_kwh = emissions.energy_consumed\n",
    "        energy_mwh = energy_kwh * 1_000          # kWh → mWh\n",
    "        energy_j   = energy_kwh * 3_600_000      # kWh → J\n",
    "        emissions_g = emissions.emissions * 1_000 * 1_000  # kg → g→ mg\n",
    "\n",
    "        results.append({\n",
    "            \"run\": run_id,\n",
    "            \"energy_mWh\": energy_mwh,\n",
    "            \"energy_J\": energy_j,\n",
    "            \"emissions_mgCO2eq\": emissions_g,\n",
    "            # \"duration_s\": emissions.duration\n",
    "        })\n",
    "\n",
    "finally:\n",
    "    tracker.stop()\n",
    "\n",
    "# --- All runs ---\n",
    "df_all = pd.DataFrame(results)\n",
    "\n",
    "# --- Valid runs (ignore first & last 2) ---\n",
    "df_valid = df_all.iloc[DISCARD_FIRST: TOTAL_RUNS - DISCARD_LAST]\n",
    "\n",
    "# --- Statistics ---\n",
    "mean = df_valid[[\"energy_mWh\", \"energy_J\", \"emissions_mgCO2eq\"]].mean()\n",
    "std  = df_valid[[\"energy_mWh\", \"energy_J\", \"emissions_mgCO2eq\"]].std()\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    [mean, std],\n",
    "    index=[\"mean\", \"std\"]\n",
    ").round(2)\n",
    "\n",
    "# --- Final table ---\n",
    "df_final = pd.concat([\n",
    "    df_valid.set_index(\"run\").round(2),\n",
    "    summary\n",
    "])\n",
    "\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining the Pytorch Lightning Trainer Configuration\n",
    "\n",
    "We will define the PyTorch Lightning Trainer configuration for fine-tuning the model.\n",
    "\n",
    "The trainer configuration includes the following parameters:\n",
    "- `max_epochs`: The maximum number of epochs to train the model.\n",
    "- `accelerator`: The accelerator to use for training: `cpu`, `gpu`, or `tpu`.\n",
    "- `devices`: The number of accelerators to use for training.\n",
    "- `logger`: The logger to use for logging the training progress.\n",
    "- `limit_*_batches`: The number of batches to use for training and validation. **This is useful for debugging and testing the model.**. You should remove these parameters when training the model for real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightning.pytorch.trainer.trainer.Trainer at 0x7fea370afa30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "## Logger\n",
    "logger = CSVLogger(save_dir=log_dir, name='cpc-finetune', version=execution_id)\n",
    "\n",
    "## Trainer\n",
    "trainer = L.Trainer(\n",
    "    # Maximum number of epochs to train\n",
    "    max_epochs=20,\n",
    "    # Training on GPU\n",
    "    accelerator=\"cpu\",\n",
    "    # We will train using 1 gpu\n",
    "    devices=1,\n",
    "    # Logger for logging\n",
    "    logger=logger,\n",
    "    # List of callbacks\n",
    "    callbacks=[checkpoint_callback],\n",
    "    # Only for testing. Remove for production. We will only train using 1 batch of training and validation\n",
    "    # limit_train_batches=1,\n",
    "    # limit_val_batches=1,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating the fine-tuning pipeline (and running the training)\n",
    "\n",
    "We will create a `SimpleLightningPipeline` to fine-tune the model. This pipeline receives the following arguments:\n",
    "- `model`: The model to train.\n",
    "- `trainer`: The PyTorch Lightning Trainer to use for training.\n",
    "- `log_dir`: The directory to save the logs, checkpoints, and other artifacts of the training process.\n",
    "- `save_run_status`: If True, save the status of the run to the log directory. This is useful for reprodutibility purposes.\n",
    "- `seed`: The seed to use for random number generators in PyTorch, NumPy, and other libraries.\n",
    "\n",
    "This pipeline is optional, as user can simply use `trainer.fit(model, datamodule)` directly to train the model. However, the pipeline provides a more organized way to train the model and save required information for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " cuda device: \n",
      "\n",
      "\n",
      " NVIDIA RTX A5000\n",
      "Log directory set to: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/supervised/logs/run_20250220-183312\n"
     ]
    }
   ],
   "source": [
    "train_pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    log_dir=log_dir,\n",
    "    save_run_status=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline to fine-tune the model on the DAGHAR dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/supervised/logs/run_20250220-183312/run_2025-02-20-18-33-13942724cbc3bb4fc0bc27800b560ec9d5.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/supervised/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | CNN_PF_Backbone  | 46.6 K | train\n",
      "1 | fc       | MLP              | 99.2 K | train\n",
      "2 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "145 K     Trainable params\n",
      "0         Non-trainable params\n",
      "145 K     Total params\n",
      "0.583     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]Using DataLoader with shuffle=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DataLoader with shuffle=True                                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 21/21 [00:00<00:00, 25.19it/s, v_num=3312, val_loss=4.080, val_acc=0.453, train_loss=0.233, train_acc=0.832]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 21/21 [00:00<00:00, 24.32it/s, v_num=3312, val_loss=4.080, val_acc=0.453, train_loss=0.233, train_acc=0.832]\n",
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/supervised/logs/run_20250220-183312/run_2025-02-20-18-33-13942724cbc3bb4fc0bc27800b560ec9d5.yaml\n"
     ]
    }
   ],
   "source": [
    "train_pipeline.run(data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Fine-tuned Model\n",
    "\n",
    "After fine-tuning the encoder on the DAGHAR dataset, we will evaluate the model's performance on the test set of the same dataset. We create a simple evaluation pipeline, that will:\n",
    "1. Run forward on test set.\n",
    "2. Calculcate the metrics. This is specified in the `classification_metrics` dictionary, where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics.\n",
    "3. Perform model analysis, such as plot t-sne embeddings.\n",
    "\n",
    "The test pipeline requires the following arguments:\n",
    "- `model`: The fine-tuned model to evaluate.\n",
    "- `trainer`: The PyTorch Lightning Trainer configuration.\n",
    "- `log_dir`: The directory to save the evaluation logs.\n",
    "- `seed`: The random seed for reproducibility.\n",
    "- `classification_metrics`: A dictionary where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics.\n",
    "- `model_analysis`: A function that performs model analysis, such as plotting t-sne embeddings.\n",
    "\n",
    "Finally, we run the evaluation pipeline with the test data module to evaluate the model on the test set. We set the `task` parameter of the `run` method to `evaluate` to evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " cuda device: \n",
      "\n",
      "\n",
      " NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/supervised/logs/run_20250220-183312\n"
     ]
    }
   ],
   "source": [
    "test_pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    log_dir=log_dir,\n",
    "    save_run_status=True,\n",
    "    seed=42,\n",
    "    classification_metrics={\n",
    "        \"accuracy\": torchmetrics.Accuracy(num_classes=6, task=\"multiclass\"),\n",
    "        \"f1\": torchmetrics.F1Score(num_classes=6, task=\"multiclass\"),\n",
    "        \"precision\": torchmetrics.Precision(num_classes=6, task=\"multiclass\"),\n",
    "        \"recall\": torchmetrics.Recall(num_classes=6, task=\"multiclass\"),\n",
    "        \"balanced_accuracy\": BalancedAccuracy(num_classes=6, task=\"multiclass\"),\n",
    "    },\n",
    "    apply_metrics_per_sample=False,\n",
    "    # model_analysis={\n",
    "    #     \"tsne\": TSNEAnalysis(\n",
    "    #         height=800,\n",
    "    #         width=800,\n",
    "    #         legend_title=\"Activity\",\n",
    "    #         title=\"t-SNE of CPC Finetuned on KuHar\",\n",
    "    #         output_filename=\"tsne_cpc_finetuned_kuhar.pdf\",\n",
    "    #         label_names={\n",
    "    #             0: \"sit\",\n",
    "    #             1: \"stand\",\n",
    "    #             2: \"walk\",\n",
    "    #             3: \"stair up\",\n",
    "    #             4: \"stair down\",\n",
    "    #             5: \"run\",\n",
    "    #             6: \"stair up and down\",\n",
    "    #         },\n",
    "    #     )\n",
    "    # },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/supervised/logs/run_20250220-183312/run_2025-02-20-18-33-309dfed83315f04d7e8f2467b3de4e3fbb.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/supervised/checkpoints/epoch=0-step=21-v6.ckpt\n",
      "Loaded model weights from the checkpoint at /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/supervised/checkpoints/epoch=0-step=21-v6.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DataLoader with shuffle=False\n",
      "Using DataLoader with shuffle=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 75.41it/s]\n",
      "Running classification metrics...\n",
      "Metrics saved to /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/supervised/logs/run_20250220-183312/metrics_2025-02-20-18-33-309dfed83315f04d7e8f2467b3de4e3fbb.yaml\n",
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/supervised/logs/run_20250220-183312/run_2025-02-20-18-33-309dfed83315f04d7e8f2467b3de4e3fbb.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/HIAAC-KR-Dev-Container/Minerva-Dev/minerva/analysis/metrics/balanced_accuracy.py:59: UserWarning: y_pred contains nan values and not all classes passed\n",
      "  warnings.warn(f\"y_pred contains nan values and not all classes passed\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classification': {'accuracy': [0.6111111044883728],\n",
       "  'f1': [0.6111111044883728],\n",
       "  'precision': [0.6111111044883728],\n",
       "  'recall': [0.6111111044883728],\n",
       "  'balanced_accuracy': [0.6609928607940674]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pipeline.run(\n",
    "    data_module, task=\"evaluate\", ckpt_path=checkpoint_callback.best_model_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
