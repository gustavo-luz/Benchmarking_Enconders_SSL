{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning From Random Projections (LFR) on the DAGHAR Dataset\n",
    "\n",
    "In this notebook, we will pre-train a CNN_PF encoder using LFR on the DAGHAR KuHAR dataset, then fine-tune it on the same dataset. Finally, we will evaluate the model's performance using a test set. Note that while this example performs both pre-training and fine-tuning on the same dataset, in practical scenarios, pre-training typically involves a larger, more diverse dataset.\n",
    "\n",
    "The following core libraries from `minerva` will be used:\n",
    "\n",
    "1. **Pre-Training**:  \n",
    "   - [`minerva.models.ssl.cpc.CPC`](): Implements CPC as a PyTorch Lightning module.\n",
    "   - [`minerva.models.nets.tnc.TSEncoder`](): Implements the TS2Vec encoder (our \"backbone\"), which will be trained using CPC (`g_enc`).\n",
    "   - [`minerva.models.nets.cpc_networks.HARCPCAutoregressive`](): Implements the CPC autoregressive network (`g_ar`), the default autoregressive network used in CPC for human activity recognition.\n",
    "   - [`minerva.data.data_modules.har_rodrigues_24.HARDataModuleCPC`](): A Lightning DataModule that loads and organizes the DAGHAR KuHAR dataset for CPC training, providing training, validation, and test data loaders.\n",
    "   - [`minerva.pipelines.lightning_pipeline.SimpleLightningPipeline`](): A wrapper for PyTorch Lightning’s `fit` method, enhancing reproducibility, logging, and customization for analytical purposes.\n",
    "2. **Fine-Tuning**:\n",
    "   - [`minerva.models.nets.base.SimpleSupervisedModel`](): Implements a supervised model with a customizable backbone (the pre-trained TS2Vec encoder) and head. This setup allows backbone freezing and supports a flexible head structure.\n",
    "      - `FromPretrained`: Loads only the backbone (TS2Vec) from the checkpoint saved during pre-training.\n",
    "      - [`minerva.models.nets.mlp.MLP`](): Defines a simple MLP classifier used as the head of the supervised model.\n",
    "   - [`minerva.pipelines.lightning_pipeline.SimpleLightningPipeline`](): The same pipeline wrapper used in pre-training, with additional support for custom evaluation metrics and analysis.\n",
    "   - [`minerva.data.data_modules.har.MultiModalHARSeriesDataModule`](): A Lightning DataModule for loading data in the format required for fine-tuning, organized similarly to the DAGHAR dataset. This module provides sliding window time series data suitable for fine-tuning.\n",
    "\n",
    "**Note**: \n",
    "1. Although we use the DAGHAR dataset here, this pipeline is flexible and can be adapted for other datasets. \n",
    "2. The data module configuration differs between pre-training (using full time series for each sample) and fine-tuning (using sliding windows) to suit model requirements at each stage.\n",
    "\n",
    "### Useful Links:\n",
    "- [DAGHAR Dataset on Zenodo](https://zenodo.org/records/13987073)\n",
    "- [Contrastive Predictive Coding for Human Activity Recognition]()\n",
    "- [TS2Vec: Towards Universal Representation of Time Series](https://cdn.aaai.org/ojs/20881/20881-13-24894-1-2-20220628.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from minerva.data.data_modules.har_rodrigues_24 import HARDataModuleCPC\n",
    "\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "from minerva.models.nets.base import SimpleSupervisedModel\n",
    "\n",
    "from minerva.models.nets.tnc import TSEncoder\n",
    "import torchmetrics\n",
    "\n",
    "from minerva.data.data_modules.har import MultiModalHARSeriesDataModule\n",
    "from minerva.models.loaders import FromPretrained\n",
    "from minerva.models.nets.base import SimpleSupervisedModel\n",
    "from minerva.models.nets.mlp import MLP\n",
    "from minerva.analysis.metrics.balanced_accuracy import BalancedAccuracy\n",
    "from minerva.analysis.model_analysis import TSNEAnalysis\n",
    "from minerva.models.nets.tnc import RnnEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ID: run_20260127-023119\n",
      "Log dir: ./logs/run_20260127-023119\n"
     ]
    }
   ],
   "source": [
    "execution_id = f'run_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "log_dir = f\"./logs/{execution_id}\" \n",
    "\n",
    "print(f\"Execution ID: {execution_id}\")\n",
    "print(f\"Log dir: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training with LFR\n",
    "\n",
    "We will train a TS2Vec Encoder using [DAGHAR's KuHAr dataset](https://zenodo.org/records/13987073) and Contrastive Predictive Coding (CPC) as the pretext task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining the Data Module\n",
    "\n",
    "We will use the `HARDataModuleCPC` data module to load the DAGHAR dataset for CPC training. This data module loads the data in the format required for CPC training, which includes full time series data for each sample.\n",
    "\n",
    "Required arguments:\n",
    "- `data_path`: Path to the directory containing the dataset.\n",
    "- `input_size`: The number of features in the input time series.\n",
    "- `window_size`: The size of the sliding window used to create the positive and negative samples for CPC.\n",
    "- `overlap`: The overlap between consecutive windows.\n",
    "- `batch_size`: The batch size for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HARDataModuleCPC(batch_size=64, datasets=/workspaces/HIAAC-KR-Dev-Container/shared_data/rodrigues_2024_datasets/1-1/kuhar/)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = HARDataModuleCPC(\n",
    "    data_path=\"/workspaces/HIAAC-KR-Dev-Container/shared_data/rodrigues_2024_datasets/1-1/kuhar/\",\n",
    "    input_size=6,\n",
    "    window=60,\n",
    "    overlap=60,\n",
    "    batch_size=64,\n",
    "    use_val_with_train = True\n",
    ")\n",
    "\n",
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pega os dataloader de treino\n",
    "# data_module.setup(\"fit\")\n",
    "# train_data_loader = data_module.train_dataloader()\n",
    "\n",
    "# # Obtem o primeiro batch de treino (64 amostras de 6x60)\n",
    "# first_batch = next(iter(train_data_loader))\n",
    "\n",
    "# X, y = first_batch\n",
    "# print(f\"O primeiro batch de treino tem shape X={tuple(X.shape)} e y={tuple(y.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1824\n"
     ]
    }
   ],
   "source": [
    "train_size = len(data_module.train_dataloader().dataset)\n",
    "print(f\"Train size: {train_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining the DIET Model\n",
    "\n",
    "We will create the CPC model using the TS2Vec encoder as the backbone and the HARCPCAutoregressive network as the autoregressive network.\n",
    "\n",
    "The TS2Vec encoder (`g_enc`) requires the following arguments:\n",
    "- `input_dims`: The number of features in the input time series.\n",
    "- `output_dims`: The dimensionality of the output embeddings.\n",
    "- `depth`: Number of convolutional layers.\n",
    "- `permute`: Whether to permute the input time series before passing it through the encoder.\n",
    "\n",
    "The HARCPCAutoregressive network (`g_ar`) requires the following arguments:\n",
    "- `input_size`: The dimensionality of the input embeddings.\n",
    "- `hidden_size`: The dimensionality of the hidden state in the autoregressive network.\n",
    "- `batch_first`: Whether the input is batch-first.\n",
    "- `bidirectional`: Whether the autoregressive network is bidirectional.\n",
    "\n",
    "The `CPC` model requires the following arguments:\n",
    "- `g_enc`: The TS2Vec encoder.\n",
    "- `g_ar`: The CPC autoregressive network.\n",
    "- `prediction_head_in_channels`: The dimensionality of the input to the prediction head.\n",
    "- `prediction_head_out_channels`: The dimensionality of the output of the prediction head.\n",
    "- `num_steps_prediction`: The number of steps to predict in the future.\n",
    "- `batch_size`: The batch size for training.\n",
    "- `minimum_steps`: The minimum number of steps in the future to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LearnFromRandomnessModel(\n",
       "  (backbone): CNN_PF_Backbone(\n",
       "    (first_padder): ZeroPadder2D(pad_at=(3,), padding_size=2)\n",
       "    (upper_part): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (lower_part): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (middle_part): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (shared_part): Sequential(\n",
       "      (0): Conv2d(48, 64, kernel_size=(3, 5), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (projectors): LFR_HAR_Projector_List(\n",
       "    (0-59): 60 x LFR_HAR_Projector(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(6, 16, kernel_size=(8,), stride=(1,), padding=(4,), bias=False)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (3): Conv1d(16, 32, kernel_size=(8,), stride=(1,), padding=(4,), bias=False)\n",
       "        (4): ReLU()\n",
       "        (5): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=544, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predictors): LFR_HAR_Predictor_List(\n",
       "    (0-59): 60 x LFR_HAR_Predictor(\n",
       "      (model): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): BatchWiseBarlowTwinLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minerva.models.nets.time_series.cnns import CNN_PF_Backbone\n",
    "from minerva.models.ssl.lfr import LearnFromRandomnessModel\n",
    "from minerva.models.nets.lfr_har_architectures import LFR_HAR_Predictor_List, LFR_HAR_Projector_List\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "# Backbone\n",
    "backbone = CNN_PF_Backbone(include_middle=True,flatten=True)\n",
    "backbone_encoding_size = 768 # Output from the backbone\n",
    "\n",
    "# LFR - from paper\n",
    "lfr_learning_rate = 3e-4\n",
    "lfr_weight_Decay = 3e-4\n",
    "lfr_predictor_training_epochs = 5\n",
    "lfr_total_projections = 6\n",
    "\n",
    "# lfr_projectors = ModuleList([LFR_HAR_Projector(size=60,encoding_size=backbone_encoding_size, input_channel=6,middle_dim=544) for _ in range(lfr_total_projections)])\n",
    "# lfr_predictors = ModuleList([LFR_HAR_Predictor(size=60,encoding_size=backbone_encoding_size, middle_dim=128, num_layers=1) for _ in range(lfr_total_projections)])\n",
    "\n",
    "\n",
    "\n",
    "lfr_projectors = LFR_HAR_Projector_List(size=60,encoding_size=backbone_encoding_size, input_channel=6,middle_dim=544)\n",
    "lfr_predictors = LFR_HAR_Predictor_List(size=60,encoding_size=backbone_encoding_size, middle_dim=128, num_layers=1)\n",
    "\n",
    "\n",
    "model = LearnFromRandomnessModel(\n",
    "    backbone=backbone,\n",
    "    projectors=lfr_projectors,\n",
    "    predictors=lfr_predictors,\n",
    "    loss_fn=None,\n",
    "    flatten=False,\n",
    "    predictor_training_epochs=lfr_predictor_training_epochs,\n",
    "    learning_rate=lfr_learning_rate,\n",
    "    weight_decay=lfr_weight_Decay,\n",
    "    max_backbone_training_steps=None,\n",
    "    num_targets= lfr_total_projections\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from minerva.models.nets.time_series.cnns import CNN_PF_Backbone\n",
    "# from minerva.models.ssl.lfr import LearnFromRandomnessModel\n",
    "# from minerva.models.nets.lfr_har_architectures import LFR_HAR_Predictor_List, LFR_HAR_Projector_List\n",
    "# from torch.nn import ModuleList\n",
    "\n",
    "# # Backbone\n",
    "# backbone = CNN_PF_Backbone(include_middle=True,flatten=True)\n",
    "# backbone_encoding_size = 768 # Output from the backbone\n",
    "\n",
    "# # LFR - from paper\n",
    "# lfr_learning_rate = 3e-4\n",
    "# lfr_weight_Decay = 3e-4\n",
    "# lfr_predictor_training_epochs = 5\n",
    "# lfr_total_projections = 6\n",
    "\n",
    "# # lfr_projectors = ModuleList([LFR_HAR_Projector(size=60,encoding_size=backbone_encoding_size, input_channel=6,middle_dim=544) for _ in range(lfr_total_projections)])\n",
    "# # lfr_predictors = ModuleList([LFR_HAR_Predictor(size=60,encoding_size=backbone_encoding_size, middle_dim=128, num_layers=1) for _ in range(lfr_total_projections)])\n",
    "\n",
    "\n",
    "\n",
    "# lfr_projectors = LFR_HAR_Projector_List(size=60,encoding_size=backbone_encoding_size, input_channel=6,middle_dim=544)\n",
    "# lfr_predictors = LFR_HAR_Predictor_List(size=60,encoding_size=backbone_encoding_size, middle_dim=128, num_layers=1)\n",
    "\n",
    "\n",
    "# model = LearnFromRandomnessModel(\n",
    "#     backbone=backbone,\n",
    "#     projectors=lfr_projectors,\n",
    "#     predictors=lfr_predictors,\n",
    "#     loss_fn=None,\n",
    "#     flatten=False,\n",
    "#     predictor_training_epochs=lfr_predictor_training_epochs,\n",
    "#     learning_rate=lfr_learning_rate,\n",
    "#     weight_decay=lfr_weight_Decay,\n",
    "#     max_backbone_training_steps=None\n",
    "# )\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining the Pytorch Lightning Trainer Configuration\n",
    "\n",
    "We will define the PyTorch Lightning Trainer configuration for training the CPC model. This configuration includes the following parameters:\n",
    "- `max_epochs`: The maximum number of epochs for training.\n",
    "- `accelartor`: The device to use for training (e.g., 'cuda' or 'cpu').\n",
    "- `devices`: The number (or a list) of accelerator devices to use.\n",
    "- `logger`: The logger to use for logging training metrics.\n",
    "- `callbacks`: The callbacks to use during training. Callbacks allows customizing the training loop and adding additional functionality, such as early stopping, model checkpointing, and learning rate scheduling.\n",
    "- `limit_*_batches`: The number of batches to limit the training, validation, and testing datasets. This is useful for debugging and testing the pipeline.\n",
    "\n",
    "The logger and callbacks can be customized based on the requirements of the training pipeline.\n",
    "We will use the `CSVLogger` callback to log the training metrics to a CSV file and the `ModelCheckpoint` callback to save the best model based on the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightning.pytorch.trainer.trainer.Trainer at 0x792f10e57f70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/',\n",
    "    monitor='train_loss',\n",
    "    mode='min',\n",
    "    save_last=True\n",
    ")\n",
    "max\n",
    "## Logger\n",
    "logger = CSVLogger(save_dir=log_dir, name='backbone-pretraining', version=execution_id)\n",
    "\n",
    "## Trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    # Only for testing. Remove for production. We will only train using 1 batch\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating the training pipeline (and running the training\n",
    "\n",
    "We will create a `SimpleLightningPipeline` to train the CPC model. This pipeline provides a high-level interface for training the model, logging the training metrics, and saving the best model based on the validation loss. Also, it allows customizing the evaluation metrics and analysis based on the requirements.\n",
    "\n",
    "The pipeline requires the following arguments:\n",
    "- `model`: The CPC model to train.\n",
    "- `trainer`: The PyTorch Lightning Trainer configuration.\n",
    "- `log_dir`: The directory to save the training logs and model checkpoints.\n",
    "- `seed`: The random seed for reproducibility.\n",
    "- `save_run_stats`: Whether to save the training statistics for reproducibility and analysis.\n",
    "\n",
    "Once the pipeline is created, we can call the `run` method with the data module to start the training process. The parameter `task` of `run` method should be set to:\n",
    "- `fit`: To train the model.\n",
    "- `evaluate`: To evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Seed set to: 42 **\n",
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/codecarbon/logs/run_20260127-023119/run_2026-01-27-02-31-31beaecab6.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type                    | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | backbone   | CNN_PF_Backbone         | 46.6 K | train\n",
      "1 | projectors | ModuleList              | 2.1 M  | train\n",
      "2 | predictors | ModuleList              | 3.5 M  | train\n",
      "3 | loss_fn    | BatchWiseBarlowTwinLoss | 0      | train\n",
      "---------------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "2.1 M     Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.563    Total estimated model params size (MB)\n",
      "33        Modules in train mode\n",
      "66        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1/1 [00:01<00:00,  0.87it/s, v_num=3119, val_loss=29.90, train_loss=36.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1/1 [00:03<00:00,  0.32it/s, v_num=3119, val_loss=29.90, train_loss=36.40]\n",
      "⏱️ fit took 10.87s → saved to /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/codecarbon/logs/run_20260127-023119/timings_fit.csv\n",
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/codecarbon/logs/run_20260127-023119/run_2026-01-27-02-31-31beaecab6.yaml\n"
     ]
    }
   ],
   "source": [
    "train_pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    log_dir=log_dir,\n",
    "    save_run_status=True,\n",
    "    seed=42\n",
    ")\n",
    "train_pipeline.run(data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Inspecting Checkpoints\n",
    "\n",
    "Once the training is complete, we can inspect the saved checkpoints to load the pre-trained model for fine-tuning or further analysis.\n",
    "\n",
    "Let's inspect the checkpoint file from the trained CPC model and check the weights that correspond to the TS2Vec encoder (`g_enc`).\n",
    "\n",
    "If you check the output, the checkpoint keys that starts with `g_enc` are the weights of the TS2Vec encoder. The `g_ar` weights are also saved in the checkpoint, and also the `prediction_head` weights. It is important to note that the `g_ar` and `prediction_head` weights are not used in the fine-tuning process, as we will only load the TS2Vec encoder for fine-tuning. Thus, we will only load the weights corresponding to the `g_enc` keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backbone.upper_part.0.weight',\n",
       " 'backbone.upper_part.0.bias',\n",
       " 'backbone.lower_part.0.weight',\n",
       " 'backbone.lower_part.0.bias',\n",
       " 'backbone.middle_part.0.weight',\n",
       " 'backbone.middle_part.0.bias',\n",
       " 'backbone.shared_part.0.weight',\n",
       " 'backbone.shared_part.0.bias',\n",
       " 'projectors.0.conv.0.weight',\n",
       " 'projectors.0.conv.3.weight',\n",
       " 'projectors.0.mlp.0.weight',\n",
       " 'projectors.0.mlp.0.bias',\n",
       " 'projectors.0.mlp.1.weight',\n",
       " 'projectors.0.mlp.1.bias',\n",
       " 'projectors.1.conv.0.weight',\n",
       " 'projectors.1.conv.3.weight',\n",
       " 'projectors.1.mlp.0.weight',\n",
       " 'projectors.1.mlp.0.bias',\n",
       " 'projectors.1.mlp.1.weight',\n",
       " 'projectors.1.mlp.1.bias',\n",
       " 'projectors.2.conv.0.weight',\n",
       " 'projectors.2.conv.3.weight',\n",
       " 'projectors.2.mlp.0.weight',\n",
       " 'projectors.2.mlp.0.bias',\n",
       " 'projectors.2.mlp.1.weight',\n",
       " 'projectors.2.mlp.1.bias',\n",
       " 'projectors.3.conv.0.weight',\n",
       " 'projectors.3.conv.3.weight',\n",
       " 'projectors.3.mlp.0.weight',\n",
       " 'projectors.3.mlp.0.bias',\n",
       " 'projectors.3.mlp.1.weight',\n",
       " 'projectors.3.mlp.1.bias',\n",
       " 'projectors.4.conv.0.weight',\n",
       " 'projectors.4.conv.3.weight',\n",
       " 'projectors.4.mlp.0.weight',\n",
       " 'projectors.4.mlp.0.bias',\n",
       " 'projectors.4.mlp.1.weight',\n",
       " 'projectors.4.mlp.1.bias',\n",
       " 'projectors.5.conv.0.weight',\n",
       " 'projectors.5.conv.3.weight',\n",
       " 'projectors.5.mlp.0.weight',\n",
       " 'projectors.5.mlp.0.bias',\n",
       " 'projectors.5.mlp.1.weight',\n",
       " 'projectors.5.mlp.1.bias',\n",
       " 'predictors.0.model.weight',\n",
       " 'predictors.0.model.bias',\n",
       " 'predictors.1.model.weight',\n",
       " 'predictors.1.model.bias',\n",
       " 'predictors.2.model.weight',\n",
       " 'predictors.2.model.bias',\n",
       " 'predictors.3.model.weight',\n",
       " 'predictors.3.model.bias',\n",
       " 'predictors.4.model.weight',\n",
       " 'predictors.4.model.bias',\n",
       " 'predictors.5.model.weight',\n",
       " 'predictors.5.model.bias']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = checkpoint_callback.last_model_path\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cuda\")\n",
    "ckpt = ckpt.get(\"state_dict\", ckpt)\n",
    "list(ckpt.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the Pre-trained TS2Vec Encoder\n",
    "\n",
    "After pre-training the TS2Vec encoder using CPC, we will fine-tune the encoder on the DAGHAR dataset for human activity recognition. We will use the same DAGHAR dataset but with a different data module configuration that provides sliding window time series data suitable for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining the Data Module\n",
    "\n",
    "We will use the `MultiModalHARSeriesDataModule` data module to load the DAGHAR dataset for fine-tuning. This data module loads the data in the format required for fine-tuning, which includes sliding window time series data for each sample. Thus, each sample of the dataset will be a 2-element tuple containing the time series (6x60, where 6 is the number of features and 60 is the window size) and the corresponding label.\n",
    "\n",
    "The data module requires the following arguments:\n",
    "- `data_path`: Path to the directory containing the dataset.\n",
    "- `feature_prefix`: The prefix of the columns containing the features. For each prefix, we will create a different channel with all columns that start with the prefix.\n",
    "- `label`: The name of the column containing the labels.\n",
    "- `features_as_channels`: If True, for each prefix, we will create a different channel with all columns that start with the prefix. If False, we will concatenate all columns with the same prefix into a single channel (the sample will be a tensor of 1x360 instead of 6x60).\n",
    "- `cast_to`: The data type to cast the features (float32).\n",
    "- `batch_size`: The batch size for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalHARSeriesDataModule(data_path=/workspaces/HIAAC-KR-Dev-Container/shared_data/daghar/standardized_view/KuHar, batch_size=64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = MultiModalHARSeriesDataModule(\n",
    "    data_path=\"/workspaces/HIAAC-KR-Dev-Container/shared_data/daghar/standardized_view/KuHar/\",\n",
    "    feature_prefixes=[\"accel-x\", \"accel-y\", \"accel-z\", \"gyro-x\", \"gyro-y\", \"gyro-z\"],\n",
    "    label=\"standard activity code\",\n",
    "    features_as_channels=True,\n",
    "    cast_to=\"float32\",\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DataLoader with shuffle=True\n",
      "Using DataLoader with shuffle=False\n",
      "torch.Size([64, 6, 60]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Pega os dataloaders de treino e validação\n",
    "data_module.setup(\"fit\")\n",
    "train_data_loader = data_module.train_dataloader()\n",
    "validation_data_loader = data_module.val_dataloader()\n",
    "first_batch = next(iter(train_data_loader))\n",
    "\n",
    "X, y = first_batch\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = CNN_PF_Backbone(include_middle=True,flatten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining the Fine-tuning Model\n",
    "\n",
    "We first load the pre-trained TS2Vec encoder from the checkpoint saved during pre-training. We then create the supervised model with the TS2Vec encoder as the backbone and an MLP classifier as the head.\n",
    "\n",
    "To load the TS2Vec encoder from the checkpoint, we use the `FromPretrained` class, which loads only the backbone from the checkpoint. The `FromPretrained` class requires the following arguments:\n",
    "- `model`: The encoder model, with randomly initialized weights.\n",
    "- `ckpt_path`: The path to the checkpoint file.\n",
    "- `filter_keys`: A list of keys to filter from checkpoint keys. We will use this argument to load only the weights corresponding to the TS2Vec encoder (The keys that start with `g_enc`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing key renaming with: {'backbone.': ''}\n",
      "\tRenaming key: backbone.upper_part.0.weight -> upper_part.0.weight (changed: True)\n",
      "\tRenaming key: backbone.upper_part.0.bias -> upper_part.0.bias (changed: True)\n",
      "\tRenaming key: backbone.lower_part.0.weight -> lower_part.0.weight (changed: True)\n",
      "\tRenaming key: backbone.lower_part.0.bias -> lower_part.0.bias (changed: True)\n",
      "\tRenaming key: backbone.middle_part.0.weight -> middle_part.0.weight (changed: True)\n",
      "\tRenaming key: backbone.middle_part.0.bias -> middle_part.0.bias (changed: True)\n",
      "\tRenaming key: backbone.shared_part.0.weight -> shared_part.0.weight (changed: True)\n",
      "\tRenaming key: backbone.shared_part.0.bias -> shared_part.0.bias (changed: True)\n",
      "Model loaded from /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/codecarbon/checkpoints/epoch=2-step=3.ckpt\n"
     ]
    }
   ],
   "source": [
    "# g_enc = TSEncoder(input_dims=6, output_dims=64, hidden_dims=64, depth=10, permute=True)\n",
    "backbone = FromPretrained(\n",
    "    model=model_,#g_enc,\n",
    "    ckpt_path=checkpoint_callback.best_model_path,\n",
    "    filter_keys=[\"backbone\"],\n",
    "    keys_to_rename={\"backbone.\": \"\"}, \n",
    "    # keys_to_rename= { \"g_enc.\":\"\",\"g_ar.\":\"\"},\n",
    "    strict=True,\n",
    "    error_on_missing_keys=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once backbone is loaded, we create the supervised model with the TS2Vec encoder as the backbone and an MLP classifier as the head. The MLP classifier requires the following arguments:\n",
    "- A list with layer sizes for the MLP. The first element should be the size of the input layer (the output of the TS2Vec encoder), and the last element should be the size of the output layer (the number of classes). We use a MLP with 3840 input units (the output of the TS2Vec encoder), a hidden layer with 128 units, and an output layer with 6 units (the number of classes in the DAGHAR dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = MLP([768, 128, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the `SimpleSupervisedModel` class to create the supervised model with the TS2Vec encoder as the backbone and the MLP classifier as the head. The `SimpleSupervisedModel` class requires the following arguments:\n",
    "- `backbone`: The backbone model (the pre-trained TS2Vec encoder).\n",
    "- `fc`: The head model (the MLP classifier).\n",
    "- `loss_fn`: The loss function to use for training (CrossEntropyLoss).\n",
    "- `flatten`: Whether to flatten the input before passing it through the head. Usually, the input is flattened if the backbone outputs a tensor with more than two dimensions.\n",
    "- `train_metrics`: A dictionary where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics.\n",
    "- `val_metrics`: A dictionary where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SimpleSupervisedModel(\n",
    "#     backbone=backbone,\n",
    "#     fc=head,\n",
    "#     loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "#     flatten=False,\n",
    "#     train_metrics={\n",
    "#         \"acc\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=6),\n",
    "#     },\n",
    "#     val_metrics={\n",
    "#         \"acc\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=6),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# model\n",
    "\n",
    "\n",
    "# from minerva.models.adapters import MaxPoolingTransposingSqueezingAdapter\n",
    "\n",
    "# adapter = MaxPoolingTransposingSqueezingAdapter(kernel_size=64)\n",
    "model = SimpleSupervisedModel(\n",
    "    backbone=backbone,\n",
    "    fc=head,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    flatten=False,\n",
    "    # adapter = adapter,\n",
    "    train_metrics={\n",
    "        \"acc\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=6),\n",
    "    },\n",
    "    val_metrics={\n",
    "        \"acc\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=6),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters: 145,830\n",
      "Trainable model parameters: 145,830\n",
      "\n",
      "Backbone:\n",
      "  Total parameters: 46,624\n",
      "  Trainable parameters: 46,624\n",
      "\n",
      "Classification Head:\n",
      "  Total parameters: 99,206\n",
      "  Trainable parameters: 99,206\n",
      "\n",
      "Verification:\n",
      "Backbone + Head total: 145,830\n",
      "Should match total: 145,830\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "MACs: 1,673,856,000.0\n",
      "Parameters: 145,830.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1: Measure ALL model parameters (including backbone AND head)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total model parameters: {total_params:,}\")\n",
    "print(f\"Trainable model parameters: {trainable_params:,}\")\n",
    "\n",
    "# Option 2: Measure components separately (as you intended)\n",
    "# Backbone parameters\n",
    "backbone_total = sum(p.numel() for p in model.backbone.parameters())\n",
    "backbone_trainable = sum(p.numel() for p in model.backbone.parameters() if p.requires_grad)\n",
    "\n",
    "# Head parameters (use model.fc, not head variable)\n",
    "head_total = sum(p.numel() for p in model.fc.parameters())\n",
    "head_trainable = sum(p.numel() for p in model.fc.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nBackbone:\")\n",
    "print(f\"  Total parameters: {backbone_total:,}\")\n",
    "print(f\"  Trainable parameters: {backbone_trainable:,}\")\n",
    "\n",
    "print(f\"\\nClassification Head:\")\n",
    "print(f\"  Total parameters: {head_total:,}\")\n",
    "print(f\"  Trainable parameters: {head_trainable:,}\")\n",
    "\n",
    "# Verify the sum matches Option 1\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Backbone + Head total: {backbone_total + head_total:,}\")\n",
    "print(f\"Should match total: {total_params:,}\")\n",
    "\n",
    "from thop import profile\n",
    "evaluation_data = torch.rand(1000, 6, 60, device='cuda')\n",
    "model.to('cuda')\n",
    "macs, params = profile(model, inputs=(evaluation_data,))\n",
    "\n",
    "print(f\"MACs: {macs:,}\")\n",
    "print(f\"Parameters: {params:,}\")\n",
    "\n",
    "# from codecarbon import EmissionsTracker\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 02:31:45] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 02:31:45] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 02:31:45] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 02:31:46] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 02:31:46] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz\n",
      "[codecarbon WARNING @ 02:31:46] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 02:31:46] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 02:31:46] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 02:31:46] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 02:31:47] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 02:31:47]   Platform system: Linux-6.8.0-65-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 02:31:47]   Python version: 3.10.6\n",
      "[codecarbon INFO @ 02:31:47]   CodeCarbon version: 3.2.1\n",
      "[codecarbon INFO @ 02:31:47]   Available RAM : 62.764 GB\n",
      "[codecarbon INFO @ 02:31:47]   CPU count: 12 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 02:31:47]   CPU model: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz\n",
      "[codecarbon INFO @ 02:31:47]   GPU count: 1\n",
      "[codecarbon INFO @ 02:31:47]   GPU model: 1 x NVIDIA RTX 5000 Ada Generation\n",
      "[codecarbon INFO @ 02:31:50] Energy consumed for RAM : 0.000003 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:31:51] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.032870376000002 W\n",
      "[codecarbon INFO @ 02:31:51] Energy consumed for All CPU : 0.000001 kWh\n",
      "[codecarbon INFO @ 02:31:51] Energy consumed for all GPUs : 0.000006 kWh. Total GPU Power : 21.457554250204904 W\n",
      "[codecarbon INFO @ 02:31:51] 0.000010 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:31:51] Energy consumed for RAM : 0.000006 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:31:52] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.009 W\n",
      "[codecarbon INFO @ 02:31:52] Energy consumed for All CPU : 0.000002 kWh\n",
      "[codecarbon INFO @ 02:31:52] Energy consumed for all GPUs : 0.000014 kWh. Total GPU Power : 28.23573695319728 W\n",
      "[codecarbon INFO @ 02:31:52] 0.000022 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:31:52] Energy consumed for RAM : 0.000008 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:31:53] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.018874368 W\n",
      "[codecarbon INFO @ 02:31:53] Energy consumed for All CPU : 0.000003 kWh\n",
      "[codecarbon INFO @ 02:31:53] Energy consumed for all GPUs : 0.000021 kWh. Total GPU Power : 24.149137400020205 W\n",
      "[codecarbon INFO @ 02:31:53] 0.000033 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:31:53] Energy consumed for RAM : 0.000011 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:31:54] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.018874368 W\n",
      "[codecarbon INFO @ 02:31:54] Energy consumed for All CPU : 0.000005 kWh\n",
      "[codecarbon INFO @ 02:31:54] Energy consumed for all GPUs : 0.000027 kWh. Total GPU Power : 23.432349785617895 W\n",
      "[codecarbon INFO @ 02:31:54] 0.000043 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:31:54] Energy consumed for RAM : 0.000014 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:31:55] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.090699264000005 W\n",
      "[codecarbon INFO @ 02:31:55] Energy consumed for All CPU : 0.000006 kWh\n",
      "[codecarbon INFO @ 02:31:55] Energy consumed for all GPUs : 0.000034 kWh. Total GPU Power : 23.6995953706877 W\n",
      "[codecarbon INFO @ 02:31:55] 0.000054 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:31:55] Energy consumed for RAM : 0.000017 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:31:56] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.007475256000001 W\n",
      "[codecarbon INFO @ 02:31:56] Energy consumed for All CPU : 0.000007 kWh\n",
      "[codecarbon INFO @ 02:31:56] Energy consumed for all GPUs : 0.000041 kWh. Total GPU Power : 23.705762082877573 W\n",
      "[codecarbon INFO @ 02:31:56] 0.000064 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:31:56] Energy consumed for RAM : 0.000020 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:31:57] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.069861528 W\n",
      "[codecarbon INFO @ 02:31:57] Energy consumed for All CPU : 0.000008 kWh\n",
      "[codecarbon INFO @ 02:31:57] Energy consumed for all GPUs : 0.000047 kWh. Total GPU Power : 23.629759995939853 W\n",
      "[codecarbon INFO @ 02:31:57] 0.000075 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:31:57] Energy consumed for RAM : 0.000023 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:31:58] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.001944000000002 W\n",
      "[codecarbon INFO @ 02:31:58] Energy consumed for All CPU : 0.000009 kWh\n",
      "[codecarbon INFO @ 02:31:58] Energy consumed for all GPUs : 0.000054 kWh. Total GPU Power : 24.357699820559436 W\n",
      "[codecarbon INFO @ 02:31:58] 0.000086 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:31:58] Energy consumed for RAM : 0.000025 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:31:59] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.144027072 W\n",
      "[codecarbon INFO @ 02:31:59] Energy consumed for All CPU : 0.000010 kWh\n",
      "[codecarbon INFO @ 02:31:59] Energy consumed for all GPUs : 0.000061 kWh. Total GPU Power : 23.347143800437717 W\n",
      "[codecarbon INFO @ 02:31:59] 0.000096 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:31:59] Energy consumed for RAM : 0.000028 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:32:00] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.042674688 W\n",
      "[codecarbon INFO @ 02:32:00] Energy consumed for All CPU : 0.000011 kWh\n",
      "[codecarbon INFO @ 02:32:00] Energy consumed for all GPUs : 0.000067 kWh. Total GPU Power : 23.09905907439401 W\n",
      "[codecarbon INFO @ 02:32:00] 0.000107 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:32:00] Energy consumed for RAM : 0.000031 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:32:01] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.306110016 W\n",
      "[codecarbon INFO @ 02:32:01] Energy consumed for All CPU : 0.000013 kWh\n",
      "[codecarbon INFO @ 02:32:01] Energy consumed for all GPUs : 0.000074 kWh. Total GPU Power : 23.15829143371037 W\n",
      "[codecarbon INFO @ 02:32:01] 0.000117 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:32:02] Energy consumed for RAM : 0.000034 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:32:02] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.006133248000001 W\n",
      "[codecarbon INFO @ 02:32:02] Energy consumed for All CPU : 0.000014 kWh\n",
      "[codecarbon INFO @ 02:32:02] Energy consumed for all GPUs : 0.000082 kWh. Total GPU Power : 27.860997104900353 W\n",
      "[codecarbon INFO @ 02:32:02] 0.000129 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:32:03] Energy consumed for RAM : 0.000037 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:32:03] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.133982424000001 W\n",
      "[codecarbon INFO @ 02:32:03] Energy consumed for All CPU : 0.000015 kWh\n",
      "[codecarbon INFO @ 02:32:03] Energy consumed for all GPUs : 0.000088 kWh. Total GPU Power : 23.780259792252153 W\n",
      "[codecarbon INFO @ 02:32:03] 0.000140 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:32:04] Energy consumed for RAM : 0.000039 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:32:04] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 8.032870376000002 W\n",
      "[codecarbon INFO @ 02:32:04] Energy consumed for All CPU : 0.000016 kWh\n",
      "[codecarbon INFO @ 02:32:04] Energy consumed for all GPUs : 0.000095 kWh. Total GPU Power : 22.899611725571415 W\n",
      "[codecarbon INFO @ 02:32:04] 0.000150 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 02:32:04] Energy consumed for RAM : 0.000039 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 02:32:05] Delta energy consumed for CPU with cpu_load : 0.000000 kWh, power : 8.015552 W\n",
      "[codecarbon INFO @ 02:32:05] Energy consumed for All CPU : 0.000016 kWh\n",
      "[codecarbon INFO @ 02:32:05] Energy consumed for all GPUs : 0.000102 kWh. Total GPU Power : 48.43988040613538 W\n",
      "[codecarbon INFO @ 02:32:05] 0.000157 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy_mWh</th>\n",
       "      <th>energy_J</th>\n",
       "      <th>emissions_mgCO2eq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.65</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>37.89</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.20</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.31</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.16</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.97</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.09</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>37.69</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>37.86</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>42.59</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.01</td>\n",
       "      <td>38.64</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      energy_mWh  energy_J  emissions_mgCO2eq\n",
       "2           0.01     38.65               1.06\n",
       "3           0.01     37.89               1.04\n",
       "4           0.01     38.20               1.04\n",
       "5           0.01     38.31               1.05\n",
       "6           0.01     38.16               1.04\n",
       "7           0.01     38.97               1.06\n",
       "8           0.01     38.09               1.04\n",
       "9           0.01     37.69               1.03\n",
       "10          0.01     37.86               1.03\n",
       "11          0.01     42.59               1.16\n",
       "mean        0.01     38.64               1.06\n",
       "std         0.00      1.44               0.04"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "TOTAL_RUNS = 14\n",
    "DISCARD_FIRST = 2\n",
    "DISCARD_LAST = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "tracker = EmissionsTracker(\n",
    "    project_name=\"basic_measurement\",\n",
    "    measure_power_secs=10,\n",
    "    save_to_file=False\n",
    ")\n",
    "\n",
    "try:\n",
    "    for run_id in range(TOTAL_RUNS):\n",
    "        tracker.start_task(f\"measure_inference_{run_id}\")\n",
    "\n",
    "        _ = model(evaluation_data)  # inference\n",
    "\n",
    "        emissions = tracker.stop_task()\n",
    "\n",
    "        energy_kwh = emissions.energy_consumed\n",
    "        energy_mwh = energy_kwh * 1_000          # kWh → mWh\n",
    "        energy_j   = energy_kwh * 3_600_000      # kWh → J\n",
    "        emissions_g = emissions.emissions * 1_000 * 1_000  # kg → g→ mg\n",
    "\n",
    "        results.append({\n",
    "            \"run\": run_id,\n",
    "            \"energy_mWh\": energy_mwh,\n",
    "            \"energy_J\": energy_j,\n",
    "            \"emissions_mgCO2eq\": emissions_g,\n",
    "            # \"duration_s\": emissions.duration\n",
    "        })\n",
    "\n",
    "finally:\n",
    "    tracker.stop()\n",
    "\n",
    "# --- All runs ---\n",
    "df_all = pd.DataFrame(results)\n",
    "\n",
    "# --- Valid runs (ignore first & last 2) ---\n",
    "df_valid = df_all.iloc[DISCARD_FIRST: TOTAL_RUNS - DISCARD_LAST]\n",
    "\n",
    "# --- Statistics ---\n",
    "mean = df_valid[[\"energy_mWh\", \"energy_J\", \"emissions_mgCO2eq\"]].mean()\n",
    "std  = df_valid[[\"energy_mWh\", \"energy_J\", \"emissions_mgCO2eq\"]].std()\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    [mean, std],\n",
    "    index=[\"mean\", \"std\"]\n",
    ").round(2)\n",
    "\n",
    "# --- Final table ---\n",
    "df_final = pd.concat([\n",
    "    df_valid.set_index(\"run\").round(2),\n",
    "    summary\n",
    "])\n",
    "\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining the Pytorch Lightning Trainer Configuration\n",
    "\n",
    "We will define the PyTorch Lightning Trainer configuration for fine-tuning the model. Save as for the pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightning.pytorch.trainer.trainer.Trainer at 0x7229f0a2f250>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "## Logger\n",
    "logger = CSVLogger(save_dir=log_dir, name='model-finetuning', version=execution_id)\n",
    "\n",
    "## Trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    # Only for testing. Remove for production. We will only train using 1 batch\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating the fine-tuning pipeline (and running the training)\n",
    "\n",
    "We will create a `SimpleLightningPipeline` to fine-tune the model. Save as for the pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Seed set to: 42 **\n",
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/logs/run_20250902-183903/run_2025-09-02-18-47-295bb28d3b.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | CNN_PF_Backbone  | 46.6 K | train\n",
      "1 | fc       | MLP              | 99.2 K | train\n",
      "2 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "145 K     Trainable params\n",
      "0         Non-trainable params\n",
      "145 K     Total params\n",
      "0.583     Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]Using DataLoader with shuffle=False\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Validation step 0: y_hat argmax: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'), y: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      " accuracy: 0.0\n",
      "Using DataLoader with shuffle=True                                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  7.93it/s, v_num=3903]Validation step 0: y_hat argmax: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'), y: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      " accuracy: 0.0\n",
      "Epoch 1: 100%|██████████| 1/1 [00:01<00:00,  0.88it/s, v_num=3903, val_loss=1.770, val_acc=0.000, train_loss=1.790, train_acc=0.188]Validation step 0: y_hat argmax: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'), y: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      " accuracy: 0.0\n",
      "Epoch 2: 100%|██████████| 1/1 [00:01<00:00,  0.88it/s, v_num=3903, val_loss=1.800, val_acc=0.000, train_loss=1.700, train_acc=0.156]Validation step 0: y_hat argmax: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'), y: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      " accuracy: 0.0\n",
      "Epoch 2: 100%|██████████| 1/1 [00:02<00:00,  0.39it/s, v_num=3903, val_loss=1.810, val_acc=0.000, train_loss=1.720, train_acc=0.0938]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1/1 [00:03<00:00,  0.27it/s, v_num=3903, val_loss=1.810, val_acc=0.000, train_loss=1.720, train_acc=0.0938]\n",
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/logs/run_20250902-183903/run_2025-09-02-18-47-295bb28d3b.yaml\n"
     ]
    }
   ],
   "source": [
    "train_pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    log_dir=log_dir,\n",
    "    save_run_status=True,\n",
    "    seed=42\n",
    ")\n",
    "train_pipeline.run(data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Fine-tuned Model\n",
    "\n",
    "After fine-tuning the TS2Vec encoder on the DAGHAR dataset, we will evaluate the model's performance on the test set. We create a simple evaluation pipeline, that will:\n",
    "1. Run forward on test set.\n",
    "2. Calculcate the metrics. This is specified in the `classification_metrics` dictionary, where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics.\n",
    "3. Perform model analysis, such as plot t-sne embeddings.\n",
    "\n",
    "The test pipeline requires the following arguments:\n",
    "- `model`: The fine-tuned model to evaluate.\n",
    "- `trainer`: The PyTorch Lightning Trainer configuration.\n",
    "- `log_dir`: The directory to save the evaluation logs.\n",
    "- `seed`: The random seed for reproducibility.\n",
    "- `classification_metrics`: A dictionary where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics.\n",
    "- `model_analysis`: A function that performs model analysis, such as plotting t-sne embeddings.\n",
    "\n",
    "Finally, we run the evaluation pipeline with the test data module to evaluate the model on the test set. We set the `task` parameter of the `run` method to `evaluate` to evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Seed set to: 42 **\n",
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/logs/run_20250902-183903/run_2025-09-02-18-47-46da27c0af.yaml\n",
      "Using DataLoader with shuffle=False\n",
      "🔍 True labels shape: torch.Size([144])\n",
      "🔍 Unique true classes: tensor([0, 1, 2, 3, 4, 5])\n",
      "🔍 True class distribution: tensor([24, 24, 24, 24, 24, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/checkpoints/epoch=0-step=1-v3.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/checkpoints/epoch=0-step=1-v3.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DataLoader with shuffle=False\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 75.30it/s] \n",
      "\n",
      "\n",
      " saving y and yhat \n",
      "\n",
      "\n",
      "🔍 Raw predictions shape: torch.Size([144, 6])\n",
      "Running classification metrics...\n",
      "🔍 Predicted classes shape: torch.Size([144])\n",
      "🔍 Unique predicted classes: tensor([2, 4, 5])\n",
      "🔍 Predicted class distribution: tensor([ 0,  0, 83,  0,  2, 59])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/HIAAC-KR-Dev-Container/Minerva-Dev/minerva/analysis/metrics/balanced_accuracy.py:60: UserWarning: y_pred contains nan values and not all classes passed\n",
      "  warnings.warn(f\"y_pred contains nan values and not all classes passed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Manual accuracy: 0.2431 (35/144)\n",
      "Running model analysis...\n",
      "Using DataLoader with shuffle=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/HIAAC-KR-Dev-Container/Minerva-Dev/minerva/analysis/model_analysis.py:196: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  X = torch.tensor(X, device=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-SNE PNG saved to /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/logs/run_20250902-183903/tsne_cpc_finetuned_motionsense.pdf\n",
      "t-SNE HTML saved to /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/logs/run_20250902-183903/tsne_cpc_finetuned_motionsense.html\n",
      "Metrics saved to /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/logs/run_20250902-183903/metrics_2025-09-02-18-47-46da27c0af.yaml\n",
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/logs/run_20250902-183903/run_2025-09-02-18-47-46da27c0af.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classification': {'accuracy': [0.2430555522441864],\n",
       "  'f1': [0.2430555522441864],\n",
       "  'precision': [0.2430555522441864],\n",
       "  'recall': [0.2430555522441864],\n",
       "  'balanced_accuracy': [0.4936355650424957]},\n",
       " 'analysis': {'tsne': {'png_path': '/workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/logs/run_20250902-183903/tsne_cpc_finetuned_motionsense.pdf',\n",
       "   'html_path': '/workspaces/HIAAC-KR-Dev-Container/Minerva-Exps/benchmarks/experiments/docs/lfr/logs/run_20250902-183903/tsne_cpc_finetuned_motionsense.html'}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    log_dir=log_dir,\n",
    "    save_run_status=True,\n",
    "    seed=42,\n",
    "    classification_metrics={\n",
    "        \"accuracy\": torchmetrics.Accuracy(num_classes=6, task=\"multiclass\"),\n",
    "        \"f1\": torchmetrics.F1Score(num_classes=6, task=\"multiclass\"),\n",
    "        \"precision\": torchmetrics.Precision(num_classes=6, task=\"multiclass\"),\n",
    "        \"recall\": torchmetrics.Recall(num_classes=6, task=\"multiclass\"),\n",
    "        \"balanced_accuracy\": BalancedAccuracy(num_classes=6, task=\"multiclass\"),\n",
    "    },\n",
    "    apply_metrics_per_sample=False,\n",
    "    model_analysis={\n",
    "        \"tsne\": TSNEAnalysis(\n",
    "            height=800,\n",
    "            width=800,\n",
    "            legend_title=\"Activity\",\n",
    "            title=\"t-SNE of CPC Finetuned on MotionSense\",\n",
    "            output_filename=\"tsne_cpc_finetuned_motionsense.pdf\",\n",
    "            label_names={\n",
    "                0: \"sit\",\n",
    "                1: \"stand\",\n",
    "                2: \"walk\",\n",
    "                3: \"stair up\",\n",
    "                4: \"stair down\",\n",
    "                5: \"run\",\n",
    "                6: \"stair up and down\",\n",
    "            },\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "test_pipeline.run(\n",
    "    data_module, task=\"evaluate\", ckpt_path=checkpoint_callback.best_model_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
